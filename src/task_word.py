import json
import sys
import os
import pickle
import string
import re
import networkx as nx
import string
from collections import defaultdict


from data_loader import PlainDataLoader


def build_graph(class_name):
    """
    æ„å»ºä¸€ä¸ªå­—çš„æœ‰å‘å›¾ï¼Œè¾¹çš„æƒé‡ä¸ºå­—å¯¹åœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡
    """
    loader = PlainDataLoader()
    if class_name == "shi":
        corpus = loader.extract_from_multiple(["tangsong","yudingquantangshi","shuimotangshi"])
    else:
        corpus = loader.body_extractor(class_name)

    G = nx.DiGraph()
    edge_weights = defaultdict(int)

    # ç”¨æ»‘åŠ¨çª—å£æ„å»ºå›¾ï¼ˆä»¥å­—ä¸ºå•ä½ï¼‰
    for sentence in corpus:
        for i in range(len(sentence) - 1):
            u = sentence[i]
            v = sentence[i + 1]
            edge_weights[(u, v)] += 1

    # å°†è¾¹åŠ å…¥å›¾ä¸­
    for (u, v), w in edge_weights.items():
        G.add_edge(u, v, weight=w)

    with open(f"graph_{class_name}.pkl", "wb") as f:
        pickle.dump(G, f)
    
    # åŠ æƒ PageRankï¼Œè€ƒè™‘å‡ºç°é¢‘ç‡
    pr = nx.pagerank(G, weight='weight')

    with open(f"word_{class_name}.pkl", "wb") as f:
        pickle.dump(pr, f)

def load_graph(class_name):
    with open(f"./word_data/word_{class_name}.pkl", "rb") as f:
        pr = pickle.load(f)
    with open(f"./word_data/graph_{class_name}.pkl", "rb") as f:
        G = pickle.load(f)   
    return pr, G

def load_word_set(filepath):
    """
    ä» txt æ–‡ä»¶ä¸­åŠ è½½è¯è¯­é›†åˆï¼ŒæŒ‰ç©ºæ ¼åˆ†éš”ï¼Œæ¯è¡Œå¤šä¸ªè¯
    è¿”å› set ç±»å‹ word_set
    """
    word_set = set()
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            words = line.strip().split()
            for word in words:
                if word:
                    word_set.add(word)
    return word_set

def recommend_next_char(prefix, pr, G, word_set=None, top_k=10):
    """
    æ ¹æ®å‰ç¼€ prefix é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ï¼Œå¹¶å°è¯•ç»„æˆè¯
    å‚æ•°ï¼š
        prefix: è¾“å…¥çš„å‰ç¼€å­—ç¬¦ä¸²
        pr: å­—çš„æ¦‚ç‡å­—å…¸ï¼ˆç”¨äºæ’åºï¼‰
        G: å­—å›¾ï¼Œæ¯ä¸ªå­—è¿æ¥åˆ°å¯èƒ½çš„ä¸‹ä¸€ä¸ªå­—é›†åˆ
        word_set: ï¼ˆå¯é€‰ï¼‰è¯å…¸é›†åˆï¼Œåˆ¤æ–­æ˜¯å¦èƒ½ç»„æˆçœŸå®è¯è¯­
        top_k: è¿”å›æ•°é‡ä¸Šé™
    è¿”å›ï¼š
        æ¨èå­—åˆ—è¡¨ï¼›å¦‚è¯å…¸å¯ç”¨åˆ™åŒæ—¶è¾“å‡ºç»„æˆè¯
    """ 
    if not prefix:
        return "ğŸ˜¢(è¿˜æ²¡è¾“å…¥)"

    last_char = prefix[-1]
    if last_char not in G:
        return "ğŸ˜¢(é€‚åˆç»“æŸè¿™å¥è¯äº†)"

    chinese_punctuation = "ï¼Œã€‚ã€ï¼ï¼Ÿã€ã€‘ï¼ˆï¼‰ã€Šã€‹â€œâ€â€˜â€™ï¼šï¼›â€”â€”â€¦Â·"
    all_punctuation = set(string.punctuation + chinese_punctuation)

    # å€™é€‰å­—ï¼šä» last_char å‡ºå‘çš„å¯èƒ½å­—
    candidates = G[last_char]
    ranked = sorted(
        [(c, pr.get(c, 0)) for c in candidates],
        key=lambda x: -x[1]
    )

    # è¿‡æ»¤æ‰æ ‡ç‚¹ï¼Œä»…å– top_k ä¸ªå­—
    filtered = [char for char, _ in ranked if char not in all_punctuation]
    top_chars = filtered[:top_k]

    # è¿”å›å­—æ¨è
    char_result = "ğŸ”¡: " + "ã€".join(top_chars) if top_chars else "ğŸ˜¢(é€‚åˆç»“æŸè¿™å¥è¯äº†)"

    # è‹¥ç»™å®šè¯å…¸ï¼Œå°è¯•ç»„æˆè¯
    if word_set:
        word_suggestions = []
        for ch in top_chars: 
            i = 0
            for word in word_set:
                if ch == word[0]:
                    word_suggestions.append(word)
                    i+=1
                if i > 5:
                    break
        word_result = "\n\n å¯ç»„è¯è¯­ï¼š" + "ã€".join(word_suggestions) if word_suggestions else "\n"
    else:
        word_result = ""

    return char_result + word_result


if __name__ == "__main__":
    class_name = "chuci" 
    # build_graph(class_name)
    
    prefix = "æ˜¥å¤©çš„èŠ±"
    word_set = load_word_set("./data/songci/fenci_shi.txt")
    
    pr, G = load_graph(class_name)
    next_chars = recommend_next_char(prefix, pr, G, word_set=word_set ,top_k=10)
    print(f"å‰ç¼€ '{prefix}' çš„ä¸‹ä¸€ä¸ªå­—é¢„æµ‹ï¼š{next_chars}")